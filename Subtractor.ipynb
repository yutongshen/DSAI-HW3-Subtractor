{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB      = 0\n",
    "SUB_ADD  = 1\n",
    "MULTIPLY = 2\n",
    "\n",
    "GEN_TYPE = SUB\n",
    "\n",
    "TRAINING_SIZE = 80000\n",
    "DIGITS = 3\n",
    "ANS_DIGITS = {\n",
    "    SUB: DIGITS + 1,\n",
    "    SUB_ADD: DIGITS + 1,\n",
    "    MULTIPLY: 2 * DIGITS\n",
    "}.get(GEN_TYPE, DIGITS + 1)\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "chars = {\n",
    "    SUB: '0123456789- ',\n",
    "    SUB_ADD: '0123456789+- ',\n",
    "    MULTIPLY: '0123456789* '\n",
    "}.get(GEN_TYPE, '0123456789+-* ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 80000\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "operator = {\n",
    "    SUB: ['-'],\n",
    "    SUB_ADD: ['-', '+'],\n",
    "    MULTIPLY: ['*']\n",
    "}\n",
    "ans_switcher = {\n",
    "    '+': lambda a, b: a + b,\n",
    "    '-': lambda a, b: a - b,\n",
    "    '*': lambda a, b: a * b\n",
    "}\n",
    "ops = operator.get(GEN_TYPE, [None])\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: random.choice(range(10 ** random.choice(range(1, DIGITS + 1))))\n",
    "    g = lambda: random.choice(ops)\n",
    "    a, b, op = f(), f(), g()\n",
    "    if op == '-':\n",
    "        a, b = sorted((a, b), reverse=True)\n",
    "    key = tuple((a, b, op))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    query = '{}{}{}'.format(a, op, b).ljust(MAXLEN)\n",
    "    ans_funct = ans_switcher.get(op, lambda a, b: float('NAN'))\n",
    "    ans = str(ans_funct(a, b)).ljust(ANS_DIGITS)\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74-10   = 64  \n",
      "72-6    = 66  \n",
      "314-5   = 309 \n",
      "683-67  = 616 \n",
      "99-74   = 25  \n",
      "77-2    = 75  \n",
      "369-27  = 342 \n",
      "262-9   = 253 \n",
      "7-3     = 4   \n",
      "990-43  = 947 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(questions[i], '=', expected[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "- Size of training data:   64,000\n",
    "- Size of validation data: 16,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable:\n",
    "    def __init__(self, chars):\n",
    "        self.chars  = list(chars)\n",
    "        self.len    = len(chars)\n",
    "        self.encode = {}\n",
    "        for i, key in enumerate(self.chars):\n",
    "            self.encode[key] = np.zeros(self.len, np.float32)\n",
    "            self.encode[key][i] = 1.\n",
    "            \n",
    "    def encoder(self, C):\n",
    "        result = np.zeros((len(C), self.len))\n",
    "        for i, c in enumerate(C):\n",
    "            try:\n",
    "                result[i] = self.encode[c]\n",
    "            except:\n",
    "                pass\n",
    "        return result\n",
    "            \n",
    "    def decoder(self, x):\n",
    "        x = x.argmax(axis=-1)\n",
    "        return ''.join(self.chars[i] for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CharacterTable(chars)\n",
    "\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), np.float32)\n",
    "y = np.zeros((len(expected), ANS_DIGITS, len(chars)), np.float32)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ct.encoder(sentence)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ct.encoder(sentence)\n",
    "\n",
    "train_x = x[:18000]\n",
    "train_y = y[:18000]\n",
    "\n",
    "validation_x = x[18000:20000]\n",
    "validation_y = y[18000:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "![model](img/seq2seq.png)\n",
    "\n",
    "- Using sequence to sequence model\n",
    "- Encoder: bi-directional LSTM\n",
    "- Decoder: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddnn_user02/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 7, 12)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 512), (None, 550912      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 512)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 512), (None, 2099200     reshape_1[0][0]                  \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 lstm_2[1][1]                     \n",
      "                                                                 lstm_2[1][2]                     \n",
      "                                                                 reshape_1[0][0]                  \n",
      "                                                                 lstm_2[2][1]                     \n",
      "                                                                 lstm_2[2][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 12)           6156        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 12)           6156        lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 12)           6156        lstm_2[2][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 12)           6156        lstm_2[3][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 48)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 4, 12)        0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4, 12)        0           reshape_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,674,736\n",
      "Trainable params: 2,674,736\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 12s 656us/step - loss: 1.5753 - acc: 0.4319 - val_loss: 1.3495 - val_acc: 0.4901\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 9s 488us/step - loss: 1.1655 - acc: 0.5651 - val_loss: 1.1095 - val_acc: 0.5824\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 9s 479us/step - loss: 0.9621 - acc: 0.6405 - val_loss: 0.9572 - val_acc: 0.6376\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 9s 495us/step - loss: 0.8366 - acc: 0.6870 - val_loss: 0.8454 - val_acc: 0.6764\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 9s 483us/step - loss: 0.7125 - acc: 0.7351 - val_loss: 0.7144 - val_acc: 0.7300\n",
      "Epoch 6/100\n",
      "18000/18000 [==============================] - 9s 492us/step - loss: 0.5394 - acc: 0.8057 - val_loss: 0.5035 - val_acc: 0.8199\n",
      "Epoch 7/100\n",
      "18000/18000 [==============================] - 9s 489us/step - loss: 0.3917 - acc: 0.8636 - val_loss: 0.3783 - val_acc: 0.8724\n",
      "Epoch 8/100\n",
      "18000/18000 [==============================] - 9s 494us/step - loss: 0.2775 - acc: 0.9062 - val_loss: 0.2568 - val_acc: 0.9157\n",
      "Epoch 9/100\n",
      "18000/18000 [==============================] - 9s 500us/step - loss: 0.1851 - acc: 0.9400 - val_loss: 0.1607 - val_acc: 0.9539\n",
      "Epoch 10/100\n",
      "18000/18000 [==============================] - 9s 494us/step - loss: 0.1479 - acc: 0.9541 - val_loss: 0.1298 - val_acc: 0.9645\n",
      "Epoch 11/100\n",
      "18000/18000 [==============================] - 9s 495us/step - loss: 0.0817 - acc: 0.9802 - val_loss: 0.0990 - val_acc: 0.9709\n",
      "Epoch 12/100\n",
      "18000/18000 [==============================] - 9s 480us/step - loss: 0.0535 - acc: 0.9884 - val_loss: 0.0707 - val_acc: 0.9799\n",
      "Epoch 13/100\n",
      "18000/18000 [==============================] - 9s 482us/step - loss: 0.1140 - acc: 0.9634 - val_loss: 0.0606 - val_acc: 0.9845\n",
      "Epoch 14/100\n",
      "18000/18000 [==============================] - 9s 482us/step - loss: 0.0261 - acc: 0.9968 - val_loss: 0.0357 - val_acc: 0.9924\n",
      "Epoch 15/100\n",
      "18000/18000 [==============================] - 9s 486us/step - loss: 0.0186 - acc: 0.9975 - val_loss: 0.0441 - val_acc: 0.9880\n",
      "Epoch 16/100\n",
      "18000/18000 [==============================] - 9s 490us/step - loss: 0.0322 - acc: 0.9918 - val_loss: 0.1511 - val_acc: 0.9485\n",
      "Epoch 17/100\n",
      "18000/18000 [==============================] - 8s 452us/step - loss: 0.0698 - acc: 0.9767 - val_loss: 0.0734 - val_acc: 0.9728\n",
      "Epoch 18/100\n",
      "18000/18000 [==============================] - 8s 452us/step - loss: 0.0263 - acc: 0.9931 - val_loss: 0.0194 - val_acc: 0.9955\n",
      "Epoch 19/100\n",
      "18000/18000 [==============================] - 8s 458us/step - loss: 0.0060 - acc: 0.9998 - val_loss: 0.0138 - val_acc: 0.9976\n",
      "Epoch 20/100\n",
      "18000/18000 [==============================] - 8s 461us/step - loss: 0.0042 - acc: 0.9999 - val_loss: 0.0131 - val_acc: 0.9978\n",
      "Epoch 21/100\n",
      "18000/18000 [==============================] - 9s 476us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0111 - val_acc: 0.9980\n",
      "Epoch 22/100\n",
      "18000/18000 [==============================] - 9s 484us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 0.9980\n",
      "Epoch 23/100\n",
      "18000/18000 [==============================] - 9s 490us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9976\n",
      "Epoch 24/100\n",
      "18000/18000 [==============================] - 9s 485us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9979\n",
      "Epoch 25/100\n",
      "18000/18000 [==============================] - 9s 490us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 0.9981\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 9s 497us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9981\n",
      "Epoch 27/100\n",
      "18000/18000 [==============================] - 9s 498us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 0.9980\n",
      "Epoch 28/100\n",
      "18000/18000 [==============================] - 9s 491us/step - loss: 9.6989e-04 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9980\n",
      "Epoch 29/100\n",
      "18000/18000 [==============================] - 9s 487us/step - loss: 8.4489e-04 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9980\n",
      "Epoch 30/100\n",
      "18000/18000 [==============================] - 9s 490us/step - loss: 7.3511e-04 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 0.9983\n",
      "Epoch 31/100\n",
      "18000/18000 [==============================] - 9s 480us/step - loss: 6.4394e-04 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 0.9985\n",
      "Epoch 32/100\n",
      "18000/18000 [==============================] - 9s 496us/step - loss: 5.6451e-04 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9983\n",
      "Epoch 33/100\n",
      "18000/18000 [==============================] - 9s 474us/step - loss: 4.9766e-04 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 0.9981\n",
      "Epoch 34/100\n",
      "18000/18000 [==============================] - 9s 492us/step - loss: 4.3929e-04 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 0.9988\n",
      "Epoch 35/100\n",
      "18000/18000 [==============================] - 9s 485us/step - loss: 3.8766e-04 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9985\n",
      "Epoch 36/100\n",
      "18000/18000 [==============================] - 9s 477us/step - loss: 3.4500e-04 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9981\n",
      "Epoch 37/100\n",
      "18000/18000 [==============================] - 9s 486us/step - loss: 3.0692e-04 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9986\n",
      "Epoch 38/100\n",
      "18000/18000 [==============================] - 9s 477us/step - loss: 2.7268e-04 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9986\n",
      "Epoch 39/100\n",
      "18000/18000 [==============================] - 9s 484us/step - loss: 2.4237e-04 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9989\n",
      "Epoch 40/100\n",
      "18000/18000 [==============================] - 9s 507us/step - loss: 2.1681e-04 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9986\n",
      "Epoch 41/100\n",
      "18000/18000 [==============================] - 9s 477us/step - loss: 1.9340e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9989\n",
      "Epoch 42/100\n",
      "18000/18000 [==============================] - 9s 489us/step - loss: 1.7318e-04 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9986\n",
      "Epoch 43/100\n",
      "18000/18000 [==============================] - 9s 486us/step - loss: 1.5446e-04 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 0.9988\n",
      "Epoch 44/100\n",
      "18000/18000 [==============================] - 9s 488us/step - loss: 1.3844e-04 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9988\n",
      "Epoch 45/100\n",
      "18000/18000 [==============================] - 9s 501us/step - loss: 1.2413e-04 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9986\n",
      "Epoch 46/100\n",
      "18000/18000 [==============================] - 9s 479us/step - loss: 1.1110e-04 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9989\n",
      "Epoch 47/100\n",
      "18000/18000 [==============================] - 9s 479us/step - loss: 9.9516e-05 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9990\n",
      "Epoch 48/100\n",
      "18000/18000 [==============================] - 9s 487us/step - loss: 8.9077e-05 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9989\n",
      "Epoch 49/100\n",
      "18000/18000 [==============================] - 9s 475us/step - loss: 8.0126e-05 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9991\n",
      "Epoch 50/100\n",
      "18000/18000 [==============================] - 8s 470us/step - loss: 7.1825e-05 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9989\n",
      "Epoch 51/100\n",
      "18000/18000 [==============================] - 9s 485us/step - loss: 6.4715e-05 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9989\n",
      "Epoch 52/100\n",
      "18000/18000 [==============================] - 9s 498us/step - loss: 5.8202e-05 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9991\n",
      "Epoch 53/100\n",
      "18000/18000 [==============================] - 9s 480us/step - loss: 5.2324e-05 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9988\n",
      "Epoch 54/100\n",
      "18000/18000 [==============================] - 9s 482us/step - loss: 4.7296e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9989\n",
      "Epoch 55/100\n",
      "18000/18000 [==============================] - 9s 503us/step - loss: 4.2605e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9990\n",
      "Epoch 56/100\n",
      "18000/18000 [==============================] - 9s 494us/step - loss: 3.8356e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9990\n",
      "Epoch 57/100\n",
      "18000/18000 [==============================] - 9s 495us/step - loss: 3.4660e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9989\n",
      "Epoch 58/100\n",
      "18000/18000 [==============================] - 9s 498us/step - loss: 3.1237e-05 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "Epoch 59/100\n",
      "18000/18000 [==============================] - 9s 482us/step - loss: 2.8172e-05 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9991\n",
      "Epoch 60/100\n",
      "18000/18000 [==============================] - 9s 489us/step - loss: 2.5557e-05 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9991\n",
      "Epoch 61/100\n",
      "18000/18000 [==============================] - 9s 474us/step - loss: 2.3161e-05 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9991\n",
      "Epoch 62/100\n",
      "18000/18000 [==============================] - 9s 492us/step - loss: 2.0869e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9989\n",
      "Epoch 63/100\n",
      "18000/18000 [==============================] - 9s 494us/step - loss: 1.8910e-05 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 0.9990\n",
      "Epoch 64/100\n",
      "18000/18000 [==============================] - 9s 491us/step - loss: 1.7055e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9989\n",
      "Epoch 65/100\n",
      "18000/18000 [==============================] - 9s 491us/step - loss: 1.5466e-05 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9989\n",
      "Epoch 66/100\n",
      "18000/18000 [==============================] - 9s 480us/step - loss: 1.4079e-05 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9990\n",
      "Epoch 67/100\n",
      "18000/18000 [==============================] - 9s 489us/step - loss: 0.9993 - acc: 0.7057 - val_loss: 0.2826 - val_acc: 0.9029\n",
      "Epoch 68/100\n",
      "18000/18000 [==============================] - 9s 488us/step - loss: 0.1431 - acc: 0.9519 - val_loss: 0.2898 - val_acc: 0.8906\n",
      "Epoch 69/100\n",
      "18000/18000 [==============================] - 9s 486us/step - loss: 0.0537 - acc: 0.9852 - val_loss: 0.0288 - val_acc: 0.9933\n",
      "Epoch 70/100\n",
      "18000/18000 [==============================] - 9s 487us/step - loss: 0.0149 - acc: 0.9973 - val_loss: 0.0285 - val_acc: 0.9918\n",
      "Epoch 71/100\n",
      "18000/18000 [==============================] - 9s 484us/step - loss: 0.0552 - acc: 0.9827 - val_loss: 0.0718 - val_acc: 0.9753\n",
      "Epoch 72/100\n",
      "18000/18000 [==============================] - 9s 484us/step - loss: 0.0109 - acc: 0.9979 - val_loss: 0.0126 - val_acc: 0.9976\n",
      "Epoch 73/100\n",
      "18000/18000 [==============================] - 9s 489us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0095 - val_acc: 0.9986\n",
      "Epoch 74/100\n",
      "18000/18000 [==============================] - 9s 492us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 0.9990\n",
      "Epoch 75/100\n",
      "18000/18000 [==============================] - 9s 493us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 0.9990\n",
      "Epoch 76/100\n",
      "18000/18000 [==============================] - 9s 496us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9990\n",
      "Epoch 77/100\n",
      "18000/18000 [==============================] - 9s 483us/step - loss: 8.7450e-04 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9990\n",
      "Epoch 78/100\n",
      "18000/18000 [==============================] - 9s 489us/step - loss: 7.3478e-04 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 0.9990\n",
      "Epoch 79/100\n",
      "18000/18000 [==============================] - 9s 493us/step - loss: 6.2514e-04 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 0.9989\n",
      "Epoch 80/100\n",
      "18000/18000 [==============================] - 9s 485us/step - loss: 5.3544e-04 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 0.9989\n",
      "Epoch 81/100\n",
      "18000/18000 [==============================] - 9s 490us/step - loss: 4.6379e-04 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 0.9990\n",
      "Epoch 82/100\n",
      "18000/18000 [==============================] - 9s 499us/step - loss: 4.0385e-04 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 0.9990\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 9s 496us/step - loss: 3.5260e-04 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 0.9990\n",
      "Epoch 84/100\n",
      "18000/18000 [==============================] - 9s 496us/step - loss: 3.1044e-04 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 0.9989\n",
      "Epoch 85/100\n",
      "18000/18000 [==============================] - 9s 492us/step - loss: 2.7387e-04 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 0.9990\n",
      "Epoch 86/100\n",
      "18000/18000 [==============================] - 9s 497us/step - loss: 2.4276e-04 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 0.9989\n",
      "Epoch 87/100\n",
      "18000/18000 [==============================] - 9s 496us/step - loss: 2.1528e-04 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 0.9989\n",
      "Epoch 88/100\n",
      "18000/18000 [==============================] - 9s 487us/step - loss: 1.9173e-04 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 0.9988\n",
      "Epoch 89/100\n",
      "18000/18000 [==============================] - 9s 484us/step - loss: 1.7109e-04 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 0.9988\n",
      "Epoch 90/100\n",
      "18000/18000 [==============================] - 9s 489us/step - loss: 1.5261e-04 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 0.9988\n",
      "Epoch 91/100\n",
      "18000/18000 [==============================] - 9s 499us/step - loss: 1.3672e-04 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9988\n",
      "Epoch 92/100\n",
      "18000/18000 [==============================] - 9s 492us/step - loss: 1.2254e-04 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 0.9989\n",
      "Epoch 93/100\n",
      "18000/18000 [==============================] - 9s 494us/step - loss: 1.1003e-04 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 0.9989\n",
      "Epoch 94/100\n",
      "18000/18000 [==============================] - 9s 478us/step - loss: 9.8721e-05 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9989\n",
      "Epoch 95/100\n",
      "18000/18000 [==============================] - 9s 495us/step - loss: 8.8883e-05 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9989\n",
      "Epoch 96/100\n",
      "18000/18000 [==============================] - 9s 497us/step - loss: 8.0007e-05 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9989\n",
      "Epoch 97/100\n",
      "18000/18000 [==============================] - 9s 498us/step - loss: 7.2079e-05 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 0.9988\n",
      "Epoch 98/100\n",
      "18000/18000 [==============================] - 9s 495us/step - loss: 6.5013e-05 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9989\n",
      "Epoch 99/100\n",
      "18000/18000 [==============================] - 9s 493us/step - loss: 5.8703e-05 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 0.9988\n",
      "Epoch 100/100\n",
      "18000/18000 [==============================] - 9s 508us/step - loss: 5.3042e-05 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f912ccce668>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation, Lambda\n",
    "from keras.layers import Input, LSTM, TimeDistributed, Dropout, RepeatVector, Reshape, Bidirectional, Concatenate, Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import load_model\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "EMBEDDING_SIZE = 256\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "encoder_inputs = Input(shape=(MAXLEN, len(chars)))\n",
    "# encoder_inputs = Input(shape=(MAXLEN,))\n",
    "# embedding = Embedding(len(chars), EMBEDDING_SIZE)(encoder_inputs)\n",
    "encoder = Bidirectional(LSTM(HIDDEN_SIZE, return_state=True))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "states = [state_h, state_c]\n",
    "\n",
    "# decoder_inputs = RepeatVector(ANS_DIGITS)(encoder_outputs)\n",
    "# decoder_outputs = Bidirectional(LSTM(int(HIDDEN_SIZE / 2), return_sequences=True))(decoder_inputs)\n",
    "# decoder_outputs = TimeDistributed(Dense(len(chars), activation='softmax'))(decoder_outputs)\n",
    "\n",
    "# Set up the decoder, which will only process one timestep at a time.\n",
    "decoder_inputs = Reshape((1, HIDDEN_SIZE * 2))\n",
    "decoder_lstm = LSTM(HIDDEN_SIZE * 2, return_state=True)\n",
    "\n",
    "all_outputs = []\n",
    "inputs = decoder_inputs(encoder_outputs)\n",
    "\n",
    "first_decoder = True\n",
    "for _ in range(ANS_DIGITS):\n",
    "    # Run the decoder on one timestep\n",
    "    outputs, state_h, state_c = decoder_lstm(inputs,\n",
    "                                             initial_state=states)\n",
    "    \n",
    "    # Reinject the outputs as inputs for the next loop iteration\n",
    "    # as well as update the states\n",
    "    states = [state_h, state_c]\n",
    "#     inputs = decoder_inputs(outputs)\n",
    "    \n",
    "    # Store the current prediction (we will concatenate all predictions later)\n",
    "    outputs = Dense(len(chars), activation='softmax')(outputs)\n",
    "    all_outputs.append(outputs)\n",
    "    \n",
    "\n",
    "# Concatenate all predictions\n",
    "decoder_outputs = Concatenate()(all_outputs)\n",
    "decoder_outputs = Reshape((ANS_DIGITS, len(chars)))(decoder_outputs)\n",
    "decoder_outputs = Lambda(lambda x: x[:,::-1])(decoder_outputs)\n",
    "\n",
    "# Define and compile model as previously\n",
    "model = Model(encoder_inputs, decoder_outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = int(len(train_x) / 128 / 100) * 100\n",
    "\n",
    "if batch_size == 0:\n",
    "    batch_size = 100\n",
    "\n",
    "    \n",
    "model.fit(train_x, train_y, \n",
    "          batch_size=batch_size, epochs=100, \n",
    "          verbose=1, validation_data=[validation_x, validation_y])\n",
    "\n",
    "# model.fit(np.argmax(train_x, axis=2), train_y, \n",
    "#           batch_size=batch_size, epochs=100, \n",
    "#           verbose=1, validation_data=[np.argmax(validation_x, axis=2), validation_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error check\n",
      "309 / 80000\n",
      "\n",
      "Prediction     Ans\n",
      "-------------------\n",
      "190-116 = 84   74  \n",
      "870-856 = 24   14  \n",
      "100-29  = 81   71  \n",
      "503-494 = 19   9   \n",
      "167-165 = 1    2   \n",
      "803-794 = 19   9   \n",
      "288-189 = 199  99  \n",
      "253-250 = 1    3   \n",
      "991-101 = 880  890 \n",
      "209-100 = 909  109 \n",
      "936-736 = 100  200 \n",
      "660-659 = 11   1   \n",
      "493-401 = 82   92  \n",
      "460-389 = 72   71  \n",
      "904-898 = 5    6   \n",
      "600-596 = 10   4   \n",
      "998-309 = 699  689 \n",
      "179-130 = 48   49  \n",
      "277-179 = 998  98  \n",
      "984-300 = 584  684 \n",
      "178-174 = 1    4   \n",
      "990-978 = 2    12  \n",
      "916-716 = 100  200 \n",
      "105-98  = 8    7   \n",
      "931-829 = 10   102 \n",
      "919-895 = 25   24  \n",
      "728-727 = 2    1   \n",
      "100-91  = 8    9   \n",
      "192-120 = 82   72  \n",
      "620-613 = 8    7   \n",
      "169-130 = 38   39  \n",
      "920-419 = 401  501 \n",
      "649-610 = 38   39  \n",
      "101-92  = 89   9   \n",
      "129-110 = 18   19  \n",
      "845-838 = 8    7   \n",
      "669-650 = 1    19  \n",
      "104-98  = 5    6   \n",
      "929-832 = 987  97  \n",
      "701-509 = 182  192 \n",
      "310-269 = 42   41  \n",
      "179-174 = 15   5   \n",
      "109-60  = 48   49  \n",
      "840-759 = 82   81  \n",
      "111-106 = 6    5   \n",
      "913-906 = 8    7   \n",
      "168-167 = 11   1   \n",
      "202-100 = 10   102 \n",
      "802-787 = 25   15  \n",
      "983-982 = 0    1   \n",
      "924-905 = 29   19  \n",
      "190-115 = 85   75  \n",
      "290-226 = 65   64  \n",
      "569-540 = 28   29  \n",
      "599-400 = 999  199 \n",
      "202-195 = 8    7   \n",
      "710-519 = 181  191 \n",
      "930-889 = 51   41  \n",
      "152-143 = 99   9   \n",
      "515-415 = 10   100 \n",
      "270-171 = 999  99  \n",
      "399-312 = 77   87  \n",
      "204-89  = 105  115 \n",
      "929-905 = 25   24  \n",
      "100-99  = 11   1   \n",
      "959-930 = 28   29  \n",
      "879-861 = 17   18  \n",
      "699-680 = 11   19  \n",
      "806-689 = 107  117 \n",
      "859-810 = 48   49  \n",
      "540-499 = 51   41  \n",
      "900-874 = 16   26  \n",
      "499-480 = 11   19  \n",
      "559-559 = 00   0   \n",
      "519-490 = 38   29  \n",
      "809-800 = 7    9   \n",
      "119-90  = 28   29  \n",
      "193-129 = 65   64  \n",
      "970-909 = 51   61  \n",
      "906-897 = 8    9   \n",
      "495-492 = 2    3   \n",
      "100-94  = 5    6   \n",
      "157-156 = 11   1   \n",
      "990-698 = 291  292 \n",
      "160-158 = 12   2   \n",
      "937-900 = 38   37  \n",
      "302-293 = 19   9   \n",
      "456-359 = 977  97  \n",
      "460-269 = 181  191 \n",
      "877-709 = 158  168 \n",
      "159-140 = 11   19  \n",
      "260-239 = 11   21  \n",
      "448-349 = 999  99  \n",
      "451-356 = 955  95  \n",
      "489-450 = 38   39  \n",
      "960-938 = 21   22  \n",
      "983-923 = 50   60  \n",
      "103-94  = 19   9   \n",
      "399-391 = 1    8   \n",
      "911-900 = 21   11  \n",
      "797-710 = 77   87  \n",
      "576-377 = 299  199 \n",
      "278-183 = 995  95  \n",
      "382-318 = 65   64  \n",
      "265-262 = 1    3   \n",
      "109-80  = 28   29  \n",
      "184-182 = 1    2   \n",
      "606-297 = 319  309 \n",
      "707-706 = 0    1   \n",
      "509-470 = 49   39  \n",
      "560-198 = 361  362 \n",
      "993-801 = 182  192 \n",
      "390-298 = 91   92  \n",
      "140-140 = 90   0   \n",
      "980-900 = 90   80  \n",
      "639-620 = 18   19  \n",
      "998-910 = 78   88  \n",
      "997-809 = 178  188 \n",
      "100-97  = 4    3   \n",
      "704-489 = 205  215 \n",
      "425-418 = 8    7   \n",
      "428-329 = 999  99  \n",
      "269-220 = 48   49  \n",
      "591-101 = 480  490 \n",
      "149-110 = 38   39  \n",
      "603-599 = 1    4   \n",
      "701-392 = 319  309 \n",
      "906-888 = 8    18  \n",
      "710-649 = 51   61  \n",
      "773-709 = 65   64  \n",
      "890-298 = 591  592 \n",
      "405-399 = 1    6   \n",
      "639-631 = 7    8   \n",
      "231-209 = 12   22  \n",
      "119-60  = 58   59  \n",
      "616-516 = 10   100 \n",
      "401-396 = 1    5   \n",
      "491-390 = 10   101 \n",
      "760-298 = 461  462 \n",
      "104-95  = 19   9   \n",
      "889-710 = 178  179 \n",
      "844-744 = 10   100 \n",
      "818-720 = 988  98  \n",
      "380-377 = 4    3   \n",
      "196-105 = 81   91  \n",
      "989-810 = 178  179 \n",
      "997-912 = 75   85  \n",
      "394-390 = 1    4   \n",
      "160-129 = 41   31  \n",
      "764-664 = 10   100 \n",
      "490-391 = 9    99  \n",
      "749-741 = 7    8   \n",
      "156-155 = 11   1   \n",
      "870-774 = 996  96  \n",
      "129-80  = 48   49  \n",
      "991-701 = 280  290 \n",
      "715-714 = 2    1   \n",
      "166-164 = 1    2   \n",
      "190-108 = 83   82  \n",
      "197-194 = 1    3   \n",
      "849-830 = 18   19  \n",
      "579-571 = 7    8   \n",
      "905-789 = 106  116 \n",
      "889-820 = 68   69  \n",
      "896-716 = 170  180 \n",
      "889-802 = 77   87  \n",
      "870-776 = 944  94  \n",
      "707-698 = 19   9   \n",
      "998-262 = 836  736 \n",
      "299-297 = 1    2   \n",
      "929-915 = 15   14  \n",
      "806-797 = 19   9   \n",
      "234-234 = 90   0   \n",
      "349-341 = 7    8   \n",
      "339-310 = 28   29  \n",
      "105-96  = 8    9   \n",
      "823-822 = 2    1   \n",
      "289-192 = 98   97  \n",
      "700-569 = 141  131 \n",
      "499-412 = 77   87  \n",
      "183-149 = 44   34  \n",
      "980-449 = 431  531 \n",
      "769-740 = 28   29  \n",
      "970-698 = 271  272 \n",
      "800-679 = 111  121 \n",
      "990-379 = 711  611 \n",
      "903-889 = 4    14  \n",
      "199-197 = 1    2   \n",
      "219-210 = 8    9   \n",
      "699-602 = 98   97  \n",
      "193-120 = 83   73  \n",
      "893-501 = 382  392 \n",
      "187-184 = 1    3   \n",
      "103-97  = 5    6   \n",
      "175-172 = 1    3   \n",
      "122-119 = 4    3   \n",
      "722-599 = 133  123 \n",
      "403-395 = 1    8   \n",
      "811-804 = 8    7   \n",
      "892-701 = 181  191 \n",
      "989-908 = 91   81  \n",
      "992-702 = 280  290 \n",
      "159-151 = 1    8   \n",
      "127-126 = 11   1   \n",
      "812-709 = 10   103 \n",
      "379-350 = 28   29  \n",
      "311-212 = 999  99  \n",
      "698-611 = 77   87  \n",
      "178-175 = 1    3   \n",
      "106-97  = 19   9   \n",
      "750-559 = 181  191 \n",
      "890-798 = 91   92  \n",
      "526-519 = 8    7   \n",
      "999-913 = 76   86  \n",
      "165-161 = 1    4   \n",
      "890-808 = 73   82  \n",
      "418-321 = 977  97  \n",
      "911-812 = 999  99  \n",
      "900-749 = 141  151 \n",
      "477-278 = 299  199 \n",
      "102-95  = 8    7   \n",
      "998-842 = 166  156 \n",
      "770-673 = 977  97  \n",
      "768-760 = 7    8   \n",
      "198-197 = 11   1   \n",
      "143-138 = 6    5   \n",
      "965-958 = 8    7   \n",
      "740-239 = 401  501 \n",
      "697-597 = 10   100 \n",
      "789-781 = 7    8   \n",
      "329-290 = 38   39  \n",
      "820-739 = 82   81  \n",
      "191-158 = 43   33  \n",
      "527-527 = 00   0   \n",
      "997-717 = 270  280 \n",
      "129-90  = 38   39  \n",
      "119-70  = 48   49  \n",
      "142-137 = 6    5   \n",
      "243-240 = 1    3   \n",
      "202-193 = 19   9   \n",
      "618-521 = 977  97  \n",
      "850-298 = 551  552 \n",
      "185-149 = 46   36  \n",
      "982-970 = 1    12  \n",
      "214-114 = 10   100 \n",
      "502-393 = 119  109 \n",
      "120-119 = 91   1   \n",
      "369-361 = 7    8   \n",
      "158-158 = 10   0   \n",
      "299-201 = 88   98  \n",
      "559-510 = 48   49  \n",
      "995-910 = 95   85  \n",
      "992-801 = 181  191 \n",
      "369-200 = 669  169 \n",
      "811-790 = 22   21  \n",
      "185-182 = 13   3   \n",
      "406-398 = 1    8   \n",
      "920-913 = 8    7   \n",
      "690-608 = 83   82  \n",
      "811-712 = 999  99  \n",
      "989-900 = 79   89  \n",
      "187-182 = 1    5   \n",
      "177-174 = 1    3   \n",
      "949-853 = 966  96  \n",
      "436-436 = 90   0   \n",
      "160-99  = 71   61  \n",
      "619-590 = 38   29  \n",
      "800-569 = 241  231 \n",
      "930-829 = 10   101 \n",
      "990-989 = 2    1   \n",
      "918-823 = 955  95  \n",
      "811-799 = 11   12  \n",
      "907-589 = 308  318 \n",
      "900-639 = 271  261 \n",
      "487-400 = 77   87  \n",
      "159-154 = 15   5   \n",
      "292-200 = 82   92  \n",
      "450-389 = 62   61  \n",
      "171-169 = 1    2   \n",
      "702-393 = 319  309 \n",
      "391-289 = 10   102 \n",
      "201-199 = 1    2   \n",
      "959-951 = 7    8   \n",
      "107-98  = 8    9   \n",
      "470-372 = 988  98  \n",
      "299-295 = 1    4   \n",
      "969-874 = 955  95  \n",
      "261-207 = 55   54  \n",
      "801-770 = 32   31  \n",
      "330-269 = 62   61  \n",
      "196-194 = 1    2   \n",
      "179-175 = 1    4   \n",
      "548-540 = 7    8   \n",
      "690-598 = 91   92  \n",
      "156-156 = 10   0   \n",
      "174-172 = 1    2   \n",
      "821-722 = 999  99  \n",
      "260-206 = 55   54  \n",
      "492-392 = 10   100 \n",
      "258-159 = 999  99  \n",
      "789-740 = 48   49  \n",
      "282-228 = 55   54  \n",
      "878-781 = 977  97  \n",
      "910-888 = 21   22  \n",
      "971-872 = 999  99  \n",
      "703-694 = 19   9   \n",
      "860-818 = 32   42  \n",
      "100-98  = 1    2   \n",
      "494-390 = 10   104 \n"
     ]
    }
   ],
   "source": [
    "print('Error check')\n",
    "pred = model.predict(x)\n",
    "err = []\n",
    "for i in range(len(x)):\n",
    "    if ct.decoder(pred[i]) != ct.decoder(y[i]):\n",
    "        err.append(i)\n",
    "print(len(err), '/', len(x))\n",
    "print()\n",
    "print('Prediction'.ljust(MAXLEN + ANS_DIGITS + 3),'Ans')\n",
    "print('-' * (MAXLEN + ANS_DIGITS * 2 + 4))\n",
    "for i in err:\n",
    "    print(ct.decoder(x[i]), '=', ct.decoder(pred[i]), ct.decoder(y[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20*0 = 19  \n"
     ]
    }
   ],
   "source": [
    "q = '20*0'\n",
    "\n",
    "q_padding = q.ljust(MAXLEN)[:MAXLEN]\n",
    "test_x = ct.encoder(q_padding)\n",
    "pred_y = model.predict(test_x.reshape(-1, MAXLEN, len(chars)))\n",
    "print(q, '=', ct.decoder(pred_y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nan'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(float('NAN'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
